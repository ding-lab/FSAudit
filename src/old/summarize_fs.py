#!/usr/bin/python

# Matthew Wyczalkowski
# m.wyczalkowski@wustl.edu
# Washington University School of Medicine

# https://stackoverflow.com/questions/5574702/how-to-print-to-stderr-in-python
from __future__ import print_function
import sys

def eprint(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)

#import sys, os, gzip
import os
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# read dataframe format specific to the FSAudit Filestat format.  Read subset of columns
# dataframe in tsv format, possible gz compression
def get_df(filename):
    read_args={}
    base, ext = os.path.splitext(filename)
    if ext == ".gz": 
        print("Reading gz")
        read_args={'compression': 'gzip'}

#  1 KEEP  # volume_name
#  1 KEEP  # timestamp
#  3 SKIP  # dirname
#  4 SKIP  filename
#  5 KEEP  ext
#  6 SKIP  file_type
#  7 KEEP  file_size
#  8 KEEP  owner_name
#  9 SKIP  time_mod
# 10 SKIP  hard_links
    read_args = { **read_args, 'usecols':['volume_name','timestamp', 'ext', 'file_size', 'owner_name'] }
    df = pd.read_csv(filename, **read_args, sep='\t')
    return df

def main():
    from optparse import OptionParser
    usage_text = """usage: %prog [options] filestat_1.gz [filestat_2.gz ...]
        Process file summary output as generated by parse_fs.py and generate summary data
        """
# NOTE: This is a python / pandas implementation of this: summarize_fs.R
# Usage: Rscript summarize_fs.sh [options] in.dat out.dat
#
# Process file summary output as generated by parse_fs.py and generate summary data
# by grouping data files by owner and/or extension.  Expected columns of input:
#   volume_name, timestamp, dirname, filename, ext, file_type, file_size, owner_name, time_mod, hard_links

# Args:
# -v: verbose output
# -Z: input is a .gz file
# -a: append to out.dat rather than overwrite
# -H: skip header
# -g group.by: group by "owner_name", "ext", or "ext-owner_name" (default)
# -D bad.fs: evaluate input data in debug mode, and print out the first matching line
#   where file_size as imported is equal to the value bad_fs.  Useful for debugging
#   cases of weird filenames which break upstream parsing.  
# -V volume: add volume name as column

    parser = OptionParser(usage_text, version="$Revision: 1.2 $")
#    parser.add_option("-i", dest="infn", default="stdin", help="Input filename. .gz extensions supported")
    parser.add_option("-o", dest="outfn", default="stdout", help="Output filename. ")

    (options, params) = parser.parse_args()

    eprint("Reading input data...")
    df = pd.concat((get_df(f) for f in params), ignore_index=True)

    eprint("Processing...")
    # now collect by volume_name, timestamp, ext, owner_name and get sum and count of file_size
    # https://stackoverflow.com/questions/31569549/how-to-groupby-a-dataframe-in-pandas-and-keep-columns
    df_sum= df.groupby(['volume_name','timestamp', 'ext', 'owner_name'], as_index=False)['file_size'].sum().rename(columns={'file_size':'total_size'})
    df_count= df.groupby(['volume_name','timestamp', 'ext', 'owner_name'], as_index=False)['file_size'].count().rename(columns={'file_size':'count'})

    df_merged = pd.merge(df_sum, df_count, on=['volume_name','timestamp', 'ext', 'owner_name'])

    eprint("Writing to", options.outfn)    
    df_merged.to_csv(options.outfn, sep='\t')


if __name__ == '__main__':
    main()

