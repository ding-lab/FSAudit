# Matthew Wyczalkowski
# m.wyczalkowski@wustl.edu
# Washington University School of Medicine

# example input
#     1	file_name	/rdcw/fs1/m.wyczalkowski/Active/ProjectStorage/Analysis/20230427.SW_vs_TD/dat/call-rescuevaffilter_pindel/rescuevaffilter.cwl
#     2	file_type	directory
#     3	file_size	4096
#     4	owner_name	m.wyczalkowski
#     5	time_mod	2023-02-01 18:11:36.000000000 -0600
#     6	hard_links	3
#
# We will write all directories and their size, calculated as the size of all files under them

import sys, os, gzip
import pandas as pd
import datetime

#pd.options.mode.copy_on_write = True

def eprint(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)

def get_subdir_size(dir_name, dirsM, filesM):
#    eprint("dirsM: %s" % dirsM)
#    eprint("filesM: %s" % filesM)
    sub_files = filesM[ filesM['file_name'].str.startswith(dir_name)]
    sub_dirs = dirsM[ dirsM['dir_name'].str.startswith(dir_name)]
    size = sub_files['file_size'].sum() + sub_dirs['dir_size'].sum()  
    return size

def main():
    from optparse import OptionParser
    usage_text = """usage: %prog [options] ...
        Process list of generated by stat of file options
        """

    parser = OptionParser(usage_text, version="$Revision: 1.2 $")
    parser.add_option("-i", dest="infn", default="stdin", help="Input filename")
    parser.add_option("-o", dest="outfn", default="stdout", help="Output filename")

    (options, params) = parser.parse_args()

    eprint("Reading %s" % options.infn)
    # This is a complete list of all entries in raw FSAudit file
    all_data = pd.read_csv(options.infn, sep="\t", usecols=[0,1,2])
    eprint("Done.")

    # file_data has path and size of all regular files
    file_data = all_data.loc[ all_data['file_type'] == 'regular file' ].drop(columns="file_type")
    file_data['depth'] = file_data['file_name'].apply(lambda x: x.count('/'))
    eprint("max file depth = %d" % file_data['depth'].max())

    # dirs is a list of all directories
    dirs = all_data.loc[ all_data['file_type'] == 'directory' ].drop(columns=["file_type", "file_size"]).rename(columns={'file_name': 'dir_name'})
    dirs['depth'] = dirs['dir_name'].apply(lambda x: x.count('/'))

    maxL = dirs['depth'].max()
    eprint("MaxL = %d" % (maxL))

    # L is the current directory level we're evaluating
    # M is L+1, One level deeper
    # The size of a directory at a given level is the sum of matching subdirs at level M and matching files at level M

#First time around:
#columns alldirs: ['dir_name', 'dir_size', 'depth']
#columns dirsL: ['dir_name', 'depth', 'dir_size']

    dirsM = pd.DataFrame(columns=["dir_name", "dir_size", "depth"]) # this lists all dirs at L+1.  Starts off empty
    alldirs = pd.DataFrame(columns=["dir_name", "dir_size", "depth"]) 

# traverse tree from end
    for L in reversed(range(1,maxL+1)):
        dirsL=dirs.loc[dirs['depth'] == L]
        filesM=file_data.loc[file_data['depth'] == L+1]

        eprint("[%s]: tree depth = %d: %d files" % (datetime.datetime.now(), L, len(dirsL.index) ))
        dirsL['dir_size'] = dirsL['dir_name'].apply(lambda x: get_subdir_size(x, dirsM, filesM))

# the above yields:
#src/make_dir_map.py:83: SettingWithCopyWarning:
#A value is trying to be set on a copy of a slice from a DataFrame.
#Try using .loc[row_indexer,col_indexer] = value instead
#
#See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
#  dirsL['dir_size'] = dirsL['dir_name'].apply(lambda x: get_subdir_size(x, dirsM, filesM))

        dirsM=dirsL
        alldirs = pd.concat([alldirs, dirsL], sort=True)

    alldirs.to_csv(options.outfn, index=False, sep="\t")
    eprint("writtent to " + options.outfn)
        
        


if __name__ == '__main__':
    main()

